{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw0dCzWAmPP4"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD7myMAv_-9I"
      },
      "outputs": [],
      "source": [
        "!gcloud dataproc clusters list --region us-central1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bN7FjhC3AC6B"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJGbo-FD_8h0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%cd -q /home/dataproc\n",
        "!ls inverted_index_gcp.py\n",
        "from inverted_index_gcp import InvertedIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQrL89eU8vzb"
      },
      "outputs": [],
      "source": [
        "# These will already be installed in the testing environment so disregard the \n",
        "# amount of time (~1 minute) it takes to install. \n",
        "!pip install -q pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install -q graphframes\n",
        "\n",
        "\n",
        "\n",
        "import pyspark\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from functools import reduce\n",
        "from google.cloud import storage\n",
        "from inverted_index_gcp import *\n",
        "\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "# ''' code addition'''\n",
        "# from flask import Flask, request, jsonify\n",
        "# ''' code addition'''\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylhUZMJK-OG7"
      },
      "source": [
        "# *PySpark*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG3wQJBa_8h3"
      },
      "outputs": [],
      "source": [
        "!ls -l /usr/lib/spark/jars/graph*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G2JQbalBUSX"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from graphframes import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Odhreo132ICZ"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwDYAXvn2ICZ"
      },
      "outputs": [],
      "source": [
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())\n",
        "spark = SparkSession.builder.getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSXHMW2X2RVP"
      },
      "source": [
        "# Suppot cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S50TCQH2RVQ"
      },
      "outputs": [],
      "source": [
        "# Stopwords\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwq6C8pvpphA"
      },
      "source": [
        "# Index Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uJat8jHsFRD"
      },
      "outputs": [],
      "source": [
        "# Calculating tf\n",
        "def stemming(tokens):\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(x) for x in tokens]\n",
        "\n",
        "def word_count_b(text, id):\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    \n",
        "    dict_counter= Counter(stem)\n",
        "    new_list=[(k, (id,dict_counter[k])) for k in dict_counter if k not in all_stopwords]\n",
        "    return new_list\n",
        "def word_count_t_stem(text, id):\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    stem = stemming(tokens)\n",
        "    dict_counter = Counter(tokens)\n",
        "    return [(k, (id,dict_counter[k])) for k in dict_counter if k not in all_stopwords]\n",
        "# get list of (id, terms)\n",
        "def doc_count_b(text, id):\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    return [(id,tokens)]\n",
        "    \n",
        "# Calculating tf for title\n",
        "def word_count_t(text, id):\n",
        "    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "    dict_counter = Counter(tokens)\n",
        "    return [(k, (id,1)) for k in dict_counter if k not in all_stopwords]\n",
        "\n",
        "# Sort posting list by wiki_id\n",
        "def reduce_word_counts(unsorted_pl): return sorted(unsorted_pl, key=lambda k: k[0])\n",
        "\n",
        "# Calculate df for each token in a posting list\n",
        "def calculate_df(postings): return postings.map(lambda x: (x[0], len(x[1])))\n",
        "\n",
        "#Write to the disk all posting lists locations\n",
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "    return int(_hash(token),16) % NUM_BUCKETS\n",
        "def partition_postings_and_write(postings,bucket_name):\n",
        "    rd = postings.map(lambda x : (token2bucket_id(x[0]),(x[0],x[1])))\n",
        "    rd = rd.groupByKey()\n",
        "    return rd.map(lambda x: InvertedIndex.write_a_posting_list(x,bucket_name))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNe77xAV2RVQ"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_remove_sw(text):\n",
        "  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n",
        "  return [x for x in tokens if x not in all_stopwords]\n",
        "\n",
        "\n",
        "def calculate_DL(text, id):\n",
        "  tokens = tokenize_and_remove_sw(text)\n",
        "  return((id,len(tokens)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JphMXTS7mSaf"
      },
      "source": [
        "# Creating Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DcEJ-PSmVtb"
      },
      "outputs": [],
      "source": [
        "def createIndex(bucket_name, index_name):\n",
        "    paths=[]\n",
        "    client = storage.Client()\n",
        "    bucket_name_title = bucket_name\n",
        "    full_path = f\"gs://{bucket_name}/\"\n",
        "\n",
        "    blobs = client.list_blobs(bucket_name)\n",
        "    for b in blobs:\n",
        "        if b.name.endswith(\"parquet\"):\n",
        "            paths.append(full_path+b.name)\n",
        "\n",
        "    # Wikipidia\n",
        "    parquetFile = spark.read.parquet(*paths)\n",
        "    dict_title_id= parquetFile.select(\"id\", \"title\").rdd\n",
        "    if index_name == \"anchor\":\n",
        "        doc_pairs = parquetFile.select(\"id\",f\"{index_name}_text\").rdd \n",
        "    elif index_name == \"body\":\n",
        "        doc_pairs = parquetFile.select(\"text\", \"id\").rdd\n",
        "    else:\n",
        "        doc_pairs = parquetFile.select(f\"{index_name}\", \"id\").rdd\n",
        "\n",
        "    if index_name == \"anchor\":\n",
        "        united_anchor_text_corpus = doc_pairs.flatMap(lambda x :x[1]).groupByKey().mapValues(list).map(lambda x : (x[0],\" \".join([y for y in x[1]])))\n",
        "        word_counts = united_anchor_text_corpus.flatMap(lambda x: word_count_b(str(x[1]), x[0]))\n",
        "   \n",
        "    elif index_name == \"body\":\n",
        "        word_counts = doc_pairs.flatMap(lambda x: word_count_b(x[0], x[1]))\n",
        "    else:\n",
        "         word_counts = doc_pairs.flatMap(lambda x: word_count_t(x[0], x[1]))\n",
        "        \n",
        "    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "    \n",
        "    dl_body_rdd = None\n",
        "    doc_tf = None\n",
        "    if index_name == \"body\":\n",
        "        doc_term_body = doc_pairs.flatMap(lambda x: doc_count_b(x[0], x[1]))\n",
        "        doc_term_body = doc_term_body.flatMap(lambda x: [(x[0], (y, x[1].count(y))) for y in set(x[1])])\n",
        "        doc_term_body = doc_term_body.groupByKey()\n",
        "        doc_term_body = doc_term_body.map(lambda x: (x[0], 1/(reduce(lambda a, b: a + b[1]**2, x[1], 0))))\n",
        "        doc_term_body = doc_term_body.map(lambda x: (x[0], math.sqrt(x[1])))\n",
        "        doc_tf = dict(doc_term_body.collect())\n",
        "        \n",
        "        postings = postings.filter(lambda x: len(x[1])>50)\n",
        "        \n",
        "        dl_body_rdd = doc_pairs.map(lambda x: calculate_DL(x[0], x[1]))\n",
        "        \n",
        "\n",
        "    w2df = calculate_df(postings)\n",
        "\n",
        "    w2df_dict = w2df.collectAsMap()\n",
        "\n",
        "    posting_locs_list = partition_postings_and_write(postings, bucket_name).collect()\n",
        "\n",
        "    super_posting_locs = defaultdict(list)\n",
        "    for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
        "        if not blob.name.endswith(\"pickle\"):\n",
        "            continue\n",
        "        with blob.open(\"rb\") as f:\n",
        "            posting_locs = pickle.load(f)\n",
        "            for k, v in posting_locs.items():\n",
        "                super_posting_locs[k].extend(v)\n",
        "\n",
        "    inverted = InvertedIndex()\n",
        "    inverted.posting_locs=super_posting_locs\n",
        "    inverted.df=w2df_dict\n",
        "    inverted.title_dict = dict_title_id.collectAsMap()\n",
        "    inverted.nf = doc_tf  \n",
        "    inverted.dl = dl_body_rdd.collectAsMap()\n",
        "    inverted.write_index('.',  f'index_{index_name}')\n",
        "    index_src =  f\"index_{index_name}.pkl\"\n",
        "    index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
        "    !gsutil cp $index_src $index_dst\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh6VBhyb9o2u"
      },
      "outputs": [],
      "source": [
        "def createStemmedIndex(bucket_name, index_name):\n",
        "    paths=[]\n",
        "    client = storage.Client()\n",
        "    bucket_name_title = bucket_name\n",
        "    full_path = f\"gs://{bucket_name}/\"\n",
        "\n",
        "    blobs = client.list_blobs(bucket_name)\n",
        "    for b in blobs:\n",
        "        if b.name.endswith(\"parquet\"):\n",
        "            paths.append(full_path+b.name)\n",
        "\n",
        "    # Wikipidia\n",
        "    parquetFile = spark.read.parquet(*paths)\n",
        "    dict_title_id= parquetFile.select(\"id\", \"title\").rdd\n",
        "    doc_pairs = parquetFile.select(\"title\", \"id\").rdd\n",
        "    word_counts = doc_pairs.flatMap(lambda x: word_count_t_stem(x[0], x[1]))\n",
        "    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "    w2df = calculate_df(postings)\n",
        "    w2df_dict = w2df.collectAsMap()\n",
        "    posting_locs_list = partition_postings_and_write(postings, bucket_name).collect()\n",
        "    \n",
        "    super_posting_locs = defaultdict(list)\n",
        "    for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n",
        "        if not blob.name.endswith(\"pickle\"):\n",
        "            continue\n",
        "        with blob.open(\"rb\") as f:\n",
        "            posting_locs = pickle.load(f)\n",
        "            for k, v in posting_locs.items():\n",
        "                super_posting_locs[k].extend(v)\n",
        "    \n",
        "    inverted = InvertedIndex()\n",
        "    inverted.posting_locs=super_posting_locs\n",
        "    inverted.df=w2df_dict\n",
        "    inverted.title_dict = dict_title_id.collectAsMap()\n",
        "    inverted.write_index('.',  f'index_{index_name}')\n",
        "    index_src =  f\"index_{index_name}.pkl\"\n",
        "    index_dst = f'gs://{bucket_name}/postings_gcp/{index_src}'\n",
        "    !gsutil cp $index_src $index_dst\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Mw0dCzWAmPP4",
        "ylhUZMJK-OG7",
        "iwq6C8pvpphA",
        "3JFAqcDMqYYv"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "PySpark",
      "language": "python",
      "name": "pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}