{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJRE6WPXuc-h"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tsowXDLCmFA"
      },
      "outputs": [],
      "source": [
        "!gcloud dataproc clusters list --region us-central1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm1CFLmJCmFC"
      },
      "outputs": [],
      "source": [
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1xbvUtECmFC"
      },
      "outputs": [],
      "source": [
        "%cd -q /home/dataproc\n",
        "!ls inverted_index_gcp.py\n",
        "from inverted_index_gcp import InvertedIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBWmJzREuf9o"
      },
      "outputs": [],
      "source": [
        "# These will already be installed in the testing environment so disregard the \n",
        "# amount of time (~1 minute) it takes to install. \n",
        "!pip install -q pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install -q graphframes\n",
        "\n",
        "\n",
        "\n",
        "import pyspark\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from functools import reduce\n",
        "from google.cloud import storage\n",
        "from inverted_index_gcp import *\n",
        "\n",
        "\n",
        "import hashlib\n",
        "def _hash(s):\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "# ''' code addition'''\n",
        "# from flask import Flask, request, jsonify\n",
        "# ''' code addition'''\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GifUJ4PHCmFD"
      },
      "outputs": [],
      "source": [
        "# Stopwords\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n",
        "                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n",
        "                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n",
        "                    \"many\", \"however\", \"would\", \"became\"]\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paths=[]\n",
        "# client = storage.Client()\n",
        "bucket_name = \"sean_bucket_anchor\"\n",
        "full_path = f\"gs://{bucket_name}/\"\n",
        "\n",
        "blobs = client.list_blobs(bucket_name)\n",
        "\n",
        "for b in blobs:\n",
        "    if b.name.endswith(\"parquet\"):\n",
        "        paths.append(full_path+b.name)"
      ],
      "metadata": {
        "id": "xnq4RNG0iCxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Page rank upload"
      ],
      "metadata": {
        "id": "KPRDsLsPjB6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_graph(pages):\n",
        "  edges = pages.flatMap(lambda x: map(lambda y: (x[0], y[0]), x[1])).distinct()\n",
        "  vertices = edges.flatMap(lambda x: x).distinct().map(lambda x:(x,))\n",
        "  return edges, vertices"
      ],
      "metadata": {
        "id": "hRa9qagkiCth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parquetFile = spark.read.parquet(*paths)\n",
        "# pages_links = spark.read.parquet(\"gs://wikidata20210801_preprocessed/*\").select(\"id\", \"anchor_text\").rdd\n",
        "pages_links = parquetFile.select(\"id\", \"anchor_text\").rdd\n",
        "# construct the graph \n",
        "edges, vertices = generate_graph(pages_links)\n",
        "# compute PageRank\n",
        "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
        "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
        "g = GraphFrame(verticesDF, edgesDF)\n",
        "pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n",
        "pr = pr_results.vertices.select(\"id\", \"pagerank\")\n",
        "pr = pr.sort(col('pagerank').desc())"
      ],
      "metadata": {
        "id": "rWuVHsS-iCTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS2QD7zrhwYV"
      },
      "outputs": [],
      "source": [
        "# Store the page rank in bucket\n",
        "pandas_df = pr.toPandas()\n",
        "storage_client = storage.Client()\n",
        "bucket_name = 'sean_bucket_title'\n",
        "file_name = 'page_rank.pkl'\n",
        "bucket = storage_client.get_bucket(bucket_name)\n",
        "blob = bucket.blob(file_name)\n",
        "\n",
        "# Save the Pandas DataFrame to a pickle file\n",
        "with open('page_rank.pkl', 'wb') as f:\n",
        "    pickle.dump(pandas_df, f)\n",
        "\n",
        "# upload pickle file to GCS\n",
        "blob.upload_from_file(open('page_rank.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the pickle file from the bucket\n",
        "pr_dct = pickle.loads(bucket.get_blob('pageranks_dict.pkl').download_as_string())"
      ],
      "metadata": {
        "id": "K_DeyLz4iBYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Page view upload"
      ],
      "metadata": {
        "id": "Zt4L5RJ5i7pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n",
        "p = Path(pv_path) \n",
        "pv_name = p.name\n",
        "pv_temp = f'{p.stem}-4dedup.txt'\n",
        "pv_clean = f'{p.stem}.pkl'\n",
        "# Download the file (2.3GB) \n",
        "!wget -N $pv_path\n",
        "# Filter for English pages, and keep just two fields: article ID (3) and monthly \n",
        "# total number of page views (5). Then, remove lines with article id or page \n",
        "# view values that are not a sequence of digits.\n",
        "!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n",
        "# Create a Counter (dictionary) that sums up the pages views for the same \n",
        "# article, resulting in a mapping from article id to total page views.\n",
        "wid2pv = Counter()\n",
        "with open(pv_temp, 'rt') as f:\n",
        "  for line in f:\n",
        "    parts = line.split(' ')\n",
        "    wid2pv.update({int(parts[0]): int(parts[1])})\n",
        "# write out the counter as binary file (pickle it)\n",
        "with open(pv_clean, 'wb') as f:\n",
        "  pickle.dump(wid2pv, f)\n",
        "# read in the counter\n",
        "with open(pv_clean, 'rb') as f:\n",
        "  wid2pv = pickle.loads(f.read())"
      ],
      "metadata": {
        "id": "c1sbNivaiBfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload into title bucket our pr\n",
        "storage_client = storage.Client()\n",
        "bucket_name = 'sean_bucket_title'\n",
        "bucket = storage_client.get_bucket(bucket_name)\n",
        "pv = pickle.loads(bucket.get_blob('page_view.pkl').download_as_string())"
      ],
      "metadata": {
        "id": "FsQEW-EpiBhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Max pv = 181126232 \n",
        "## Max pr = 9913.728782160779"
      ],
      "metadata": {
        "id": "b5e9Kb3Pjtow"
      }
    }
  ]
}